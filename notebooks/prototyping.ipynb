{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "## Scope\n",
    "- Sort out completions using aisuite - `done!`\n",
    "- Create basic data structure that is RAG ready - faiss - `todo`\n",
    "- Prompt engineering (and tests) - `todo`\n",
    "- Mapping with Folium - `todo`\n",
    "- Tying together with streamlit - `todo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completions - `aisuite`\n",
    "todo:\n",
    "- pivot to .env for secret management? - just awkward as thats the present venv name - https://github.com/andrewyng/aisuite/blob/main/.env.sample\n",
    "- explore alternate anthropic models - `model = 'anthropic:claude-3-5-sonnet-v2@20241022'` \n",
    "- see how the resulting issue shapes up - https://github.com/andrewyng/aisuite/issues/155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.__init__() got an unexpected keyword argument 'proxies'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manthropic:claude-3-5-sonnet-v2@20241022\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRespond in Pirate English.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     12\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me a joke.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     13\u001b[0m ]\n\u001b[1;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Users\\geoff\\dev\\buildmas24\\.env\\Lib\\site-packages\\aisuite\\client.py:108\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mproviders:\n\u001b[0;32m    107\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mprovider_configs\u001b[38;5;241m.\u001b[39mget(provider_key, {})\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mproviders[provider_key] \u001b[38;5;241m=\u001b[39m \u001b[43mProviderFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mproviders\u001b[38;5;241m.\u001b[39mget(provider_key)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m provider:\n",
      "File \u001b[1;32mc:\\Users\\geoff\\dev\\buildmas24\\.env\\Lib\\site-packages\\aisuite\\provider.py:46\u001b[0m, in \u001b[0;36mProviderFactory.create_provider\u001b[1;34m(cls, provider_key, config)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Instantiate the provider class\u001b[39;00m\n\u001b[0;32m     45\u001b[0m provider_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, provider_class_name)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovider_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\geoff\\dev\\buildmas24\\.env\\Lib\\site-packages\\aisuite\\providers\\anthropic_provider.py:16\u001b[0m, in \u001b[0;36mAnthropicProvider.__init__\u001b[1;34m(self, **config)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    Initialize the Anthropic provider with the given configuration.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    Pass the entire configuration dictionary to the Anthropic client constructor.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43manthropic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAnthropic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\geoff\\dev\\buildmas24\\.env\\Lib\\site-packages\\anthropic\\_client.py:121\u001b[0m, in \u001b[0;36mAnthropic.__init__\u001b[1;34m(self, api_key, auth_token, base_url, timeout, max_retries, default_headers, default_query, http_client, transport, proxies, connection_pool_limits, _strict_response_validation)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.anthropic.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_pool_limits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_stream_cls \u001b[38;5;241m=\u001b[39m Stream\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletions \u001b[38;5;241m=\u001b[39m resources\u001b[38;5;241m.\u001b[39mCompletions(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\geoff\\dev\\buildmas24\\.env\\Lib\\site-packages\\anthropic\\_base_client.py:835\u001b[0m, in \u001b[0;36mSyncAPIClient.__init__\u001b[1;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    819\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    823\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[0;32m    824\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     _strict_response_validation\u001b[38;5;241m=\u001b[39m_strict_response_validation,\n\u001b[0;32m    834\u001b[0m )\n\u001b[1;32m--> 835\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\geoff\\dev\\buildmas24\\.env\\Lib\\site-packages\\anthropic\\_base_client.py:733\u001b[0m, in \u001b[0;36m_DefaultHttpxClient.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimits\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[0;32m    732\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 733\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Client.__init__() got an unexpected keyword argument 'proxies'"
     ]
    }
   ],
   "source": [
    "import aisuite as ai, toml, os\n",
    "secrets = toml.load('../secrets.toml')\n",
    "API_KEY = secrets.get('ANTHROPIC_SECRET')\n",
    "os.environ['ANTHROPIC_API_KEY'] = API_KEY\n",
    "\n",
    "\n",
    "client = ai.Client()\n",
    "model = 'anthropic:claude-3-5-haiku@20241022' \n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Respond in Pirate English.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = model,\n",
    "    messages = messages,\n",
    "    temperature=0.75\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30.1 3.13.0 (tags/v3.13.0:60403a5, Oct  7 2024, 09:38:07) [MSC v.1941 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import anthropic, sys\n",
    "print(anthropic.__version__,\n",
    "      #ai.__version__,\n",
    "      sys.version)\n",
    "# aisuite==0.1.6 via requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, here be a jest fer ye, matey!\n",
      "\n",
      "Why be a pirate's favorite letter? 'R', of course! *hearty pirate laugh*\n",
      "\n",
      "Yarrr har har! *slaps knee and takes a swig from rum bottle*\n"
     ]
    }
   ],
   "source": [
    "# upgrading to anthropic 0.40.0 fixed this issue.\n",
    "import aisuite as ai, toml, os\n",
    "secrets = toml.load('../secrets.toml')\n",
    "API_KEY = secrets.get('ANTHROPIC_SECRET')\n",
    "os.environ['ANTHROPIC_API_KEY'] = API_KEY\n",
    "\n",
    "\n",
    "client = ai.Client()\n",
    "model = 'anthropic:claude-3-5-haiku-20241022' \n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Respond in Pirate English.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = model,\n",
    "    messages = messages,\n",
    "    temperature=0.75\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completions - `anthropic` directly\n",
    "No longer necessary though it did help troubleshoot issues with aisuite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text=\"Arrr, here be a jest fer ye, me hearty!\\n\\nWhy'd the pirate make a terrible teacher? 'Cause he kept usin' his ARRRRRbitrary punishments! *hearty laugh*\\n\\n*slaps knee and takes a swig from a rum bottle*\\n\\nYarrr! That be a knee-slappin' chuckle fer ye! *winks*\", type='text')]\n"
     ]
    }
   ],
   "source": [
    "# let's pivot - to the anthropic API directly for now, and we can fix this down the track\n",
    "# https://pypi.org/project/anthropic/\n",
    "from anthropic import Anthropic\n",
    "import toml\n",
    "secrets = toml.load('../secrets.toml')\n",
    "API_KEY = secrets.get('ANTHROPIC_SECRET')\n",
    "\n",
    "client = Anthropic(\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    max_tokens=1024,\n",
    "    system=\"Respond in Pirate English.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "    ],\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore RAG and datastructures\n",
    "1. We need to firstly transform the data into embeddings or vectors against which we search. The vectors need to use the same embedding model as what we'll use at runtime. We can use anthropic, and if we do we'll need to eventually explore batching to reduce costs. OR we can introduce another library and a bunch of new models (e.g. ), which may need GPU's etc. For now, probably simplest to go with anthropic and a small subset of the data (e.g. 100 programs near my location). \n",
    "2. We then need to implement search on whatever text the user provides, to provide a number of results. I like the idea of the `faiss` library here, and maybe this also can be implemented in numpy (trying to reduce the number of dependencies if I can, and realistically the data isn't big enough to have to worry about a heavy duty library just yet)\n",
    "3. Finally, we need to add the search results as context, along with the initial user prompt, so that the response provides valid outputs. For this, I suspect some prompt engineering is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abn</th>\n",
       "      <th>charity name</th>\n",
       "      <th>how purposes were pursued</th>\n",
       "      <th>total full time equivalent staff</th>\n",
       "      <th>staff - volunteers</th>\n",
       "      <th>Program name</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Charity weblink</th>\n",
       "      <th>location_number</th>\n",
       "      <th>operating_location</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>11930852906</td>\n",
       "      <td>Kind Hearts Illawarra</td>\n",
       "      <td>We continued to run outreach programme, even a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>Outreach in the Park</td>\n",
       "      <td>Soup kitchens</td>\n",
       "      <td>www.kindheartsillawarra.com.au</td>\n",
       "      <td>1</td>\n",
       "      <td>MacCabe Park, Wollongong NSW, Australia</td>\n",
       "      <td>-34.427625</td>\n",
       "      <td>150.894013</td>\n",
       "      <td>0.294943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>11930852906</td>\n",
       "      <td>Kind Hearts Illawarra</td>\n",
       "      <td>We continued to run outreach programme, even a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>Produce Table</td>\n",
       "      <td>Food aid</td>\n",
       "      <td>www.kindheartsillawarra.com.au</td>\n",
       "      <td>1</td>\n",
       "      <td>MacCabe Park, Wollongong NSW, Australia</td>\n",
       "      <td>-34.427625</td>\n",
       "      <td>150.894013</td>\n",
       "      <td>0.294943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>11981168448</td>\n",
       "      <td>CORRIMAL RSL SUB-BRANCH LIMITED</td>\n",
       "      <td>Provide support to veterans and their families...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152</td>\n",
       "      <td>ANZAC Day Dawn Commemorative service</td>\n",
       "      <td>Unknown or not classified</td>\n",
       "      <td>https://www.rslnsw.org.au/</td>\n",
       "      <td>1</td>\n",
       "      <td>Corrimal NSW, Australia</td>\n",
       "      <td>-34.373193</td>\n",
       "      <td>150.896911</td>\n",
       "      <td>5.779034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>11981168448</td>\n",
       "      <td>CORRIMAL RSL SUB-BRANCH LIMITED</td>\n",
       "      <td>Provide support to veterans and their families...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152</td>\n",
       "      <td>Remembrance Day Commemorative Service</td>\n",
       "      <td>Unknown or not classified</td>\n",
       "      <td>http://www.rslnsw.org.au/</td>\n",
       "      <td>1</td>\n",
       "      <td>Corrimal NSW, Australia</td>\n",
       "      <td>-34.373193</td>\n",
       "      <td>150.896911</td>\n",
       "      <td>5.779034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>11981168448</td>\n",
       "      <td>CORRIMAL RSL SUB-BRANCH LIMITED</td>\n",
       "      <td>Provide support to veterans and their families...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152</td>\n",
       "      <td>RSL NSW s Charitable Purpose</td>\n",
       "      <td>Welfare</td>\n",
       "      <td>https://www.rsldefencecare.org.au</td>\n",
       "      <td>1</td>\n",
       "      <td>Corrimal NSW, Australia</td>\n",
       "      <td>-34.366667</td>\n",
       "      <td>150.891667</td>\n",
       "      <td>6.495752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             abn                     charity name  \\\n",
       "292  11930852906            Kind Hearts Illawarra   \n",
       "293  11930852906            Kind Hearts Illawarra   \n",
       "309  11981168448  CORRIMAL RSL SUB-BRANCH LIMITED   \n",
       "310  11981168448  CORRIMAL RSL SUB-BRANCH LIMITED   \n",
       "311  11981168448  CORRIMAL RSL SUB-BRANCH LIMITED   \n",
       "\n",
       "                             how purposes were pursued  \\\n",
       "292  We continued to run outreach programme, even a...   \n",
       "293  We continued to run outreach programme, even a...   \n",
       "309  Provide support to veterans and their families...   \n",
       "310  Provide support to veterans and their families...   \n",
       "311  Provide support to veterans and their families...   \n",
       "\n",
       "     total full time equivalent staff  staff - volunteers  \\\n",
       "292                               0.0                  13   \n",
       "293                               0.0                  13   \n",
       "309                               0.0                 152   \n",
       "310                               0.0                 152   \n",
       "311                               0.0                 152   \n",
       "\n",
       "                              Program name             Classification  \\\n",
       "292                   Outreach in the Park              Soup kitchens   \n",
       "293                          Produce Table                   Food aid   \n",
       "309   ANZAC Day Dawn Commemorative service  Unknown or not classified   \n",
       "310  Remembrance Day Commemorative Service  Unknown or not classified   \n",
       "311           RSL NSW s Charitable Purpose                    Welfare   \n",
       "\n",
       "                       Charity weblink  location_number  \\\n",
       "292     www.kindheartsillawarra.com.au                1   \n",
       "293     www.kindheartsillawarra.com.au                1   \n",
       "309         https://www.rslnsw.org.au/                1   \n",
       "310          http://www.rslnsw.org.au/                1   \n",
       "311  https://www.rsldefencecare.org.au                1   \n",
       "\n",
       "                          operating_location   latitude   longitude  distance  \n",
       "292  MacCabe Park, Wollongong NSW, Australia -34.427625  150.894013  0.294943  \n",
       "293  MacCabe Park, Wollongong NSW, Australia -34.427625  150.894013  0.294943  \n",
       "309                  Corrimal NSW, Australia -34.373193  150.896911  5.779034  \n",
       "310                  Corrimal NSW, Australia -34.373193  150.896911  5.779034  \n",
       "311                  Corrimal NSW, Australia -34.366667  150.891667  6.495752  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(175, 13)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's get some test data near Wollongong\n",
    "import pandas as pd, numpy as np\n",
    "df = pd.read_pickle('../data/transformed_charities.pkl')\n",
    "user_lat = -34.425072\n",
    "user_lon = 150.893143\n",
    "\n",
    "cols_of_interest = [\n",
    "    'abn',\n",
    "    'charity name',\n",
    "    'how purposes were pursued',\n",
    "    'total full time equivalent staff',\n",
    "    'staff - volunteers',\n",
    "    'Program name',\n",
    "    'Classification',\n",
    "    'Charity weblink',\n",
    "    'location_number',\n",
    "    'operating_location',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'distance' # what we're about to compute\n",
    "]\n",
    "\n",
    "# compute distance - this could be to slow for a user session\n",
    "def haversine_distance_on_df(row,user_lat,user_lon):\n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "        \n",
    "    # Convert to radians\n",
    "    user_lat_rad = np.radians(user_lat)\n",
    "    user_lon_rad = np.radians(user_lon)\n",
    "    charity_lats_rad = np.radians(row['latitude'])\n",
    "    charity_lons_rad = np.radians(row['longitude'])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = charity_lats_rad - user_lat_rad\n",
    "    dlon = charity_lons_rad - user_lon_rad\n",
    "    a = np.sin(dlat/2)**2 + np.cos(user_lat_rad) * np.cos(charity_lats_rad) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# Apply the function to create a new distance column\n",
    "df['distance'] = df.apply(lambda row: haversine_distance_on_df(row, user_lat, user_lon), axis=1)\n",
    "\n",
    "# filter data to within 10km\n",
    "filtered_data = df.loc[df.distance <=10.0,cols_of_interest]\n",
    "display(filtered_data.head(),filtered_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique identifier - that is also informative to the model\n",
    "filtered_data['id'] = filtered_data['charity name'].astype(str)+' | '+filtered_data['Program name']+' | '+filtered_data['operating_location'].astype(str)\n",
    "assert filtered_data['id'].nunique()/filtered_data['id'].count() == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo - consider adding more metatadata\n",
    "def prepare_text_for_embedding(row):\n",
    "    \"\"\"Prepare text by combining identifier and content\"\"\"\n",
    "    return f\"ID:{row['id']} - {row['how purposes were pursued']}\"\n",
    "\n",
    "filtered_data['text_to_embed'] = filtered_data.apply(prepare_text_for_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting embedding: 'Beta' object has no attribute 'embeddings'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "292    None\n",
       "Name: text_to_embed, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "import toml\n",
    "\n",
    "# Load API key from environment\n",
    "secrets = toml.load('../secrets.toml')\n",
    "API_KEY = secrets.get('ANTHROPIC_SECRET')\n",
    "client = Anthropic(api_key=API_KEY)\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"Get embedding from Anthropic API\"\"\"\n",
    "    try:\n",
    "        response = client.beta.embeddings.create(\n",
    "            model=\"claude-3-haiku-20241022\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.embedding  # Returns the embedding vector\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create embeddings for our filtered dataset\n",
    "\n",
    "result = filtered_data['text_to_embed'].head(1).apply(get_embedding)\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This was a bum steer by Claude - Anthropic do not provide embedding models just yet! https://docs.anthropic.com/en/docs/build-with-claude/embeddings\n",
    "- We could look at their suggested provider...\n",
    "- We could also just implement a traditional NLP method of TF-IDF, which saves money and can be executed by a streamlit server (no GPU needed in runtime) - https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### pivot to TF-IDF (or traditional NLP approaches) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: This is a sample document about machine learning\n",
      "Similarity: 0.5465\n",
      "\n",
      "Document: Another document discussing data science\n",
      "Similarity: 0.4795\n",
      "\n",
      "Document: A third document about artificial intelligence\n",
      "Similarity: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import logging\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, debug: bool = False):\n",
    "        self.debug = debug\n",
    "        self._setup_logging()\n",
    "        self._download_nltk_data()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def _setup_logging(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if self.debug:\n",
    "            self.logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            self.logger.setLevel(logging.WARNING)\n",
    "            \n",
    "    def _download_nltk_data(self):\n",
    "        \"\"\"Download required NLTK data if not present\"\"\"\n",
    "        for package in ['punkt', 'stopwords', 'wordnet', 'omw-1.4']:\n",
    "            try:\n",
    "                nltk.data.find(f'tokenizers/{package}')\n",
    "            except LookupError:\n",
    "                nltk.download(package, quiet=True)\n",
    "                \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess text with optional debugging\"\"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "        except LookupError:\n",
    "            tokens = text.split()\n",
    "            \n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                 if token not in self.stop_words and len(token) > 2]\n",
    "        \n",
    "        processed = ' '.join(tokens)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"Preprocessed text: {processed[:100]}...\")\n",
    "        \n",
    "        return processed\n",
    "\n",
    "class DocumentSimilarity:\n",
    "    def __init__(self, max_features: int = 5000, min_df: int = 1, \n",
    "                 max_df: float = 1.0, debug: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize document similarity processor\n",
    "        \n",
    "        Args:\n",
    "            max_features: Maximum number of features for TF-IDF\n",
    "            min_df: Minimum document frequency\n",
    "            max_df: Maximum document frequency\n",
    "            debug: Enable debug logging\n",
    "        \"\"\"\n",
    "        self.debug = debug\n",
    "        self._setup_logging()\n",
    "        \n",
    "        self.preprocessor = TextPreprocessor(debug=debug)\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            token_pattern=r'\\b\\w+\\b',\n",
    "            ngram_range=(1, 1)\n",
    "        )\n",
    "        \n",
    "    def _setup_logging(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if self.debug:\n",
    "            self.logger.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            self.logger.setLevel(logging.WARNING)\n",
    "            \n",
    "    def fit_transform_documents(self, df: pd.DataFrame, \n",
    "                              text_column: str) -> pd.DataFrame:\n",
    "        \"\"\"Process documents and create TF-IDF matrix\"\"\"\n",
    "        self.logger.debug(\"Processing documents...\")\n",
    "        \n",
    "        # Preprocess texts\n",
    "        processed_texts = df[text_column].apply(self.preprocessor.preprocess_text)\n",
    "        if self.debug:\n",
    "            self.logger.debug(\"\\nSample processed texts:\")\n",
    "            for text in processed_texts.head(3):\n",
    "                self.logger.debug(f\"{text[:100]}...\")\n",
    "        \n",
    "        # Fit and transform\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(processed_texts)\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"\\nVocabulary size: {len(feature_names)}\")\n",
    "            self.logger.debug(f\"Sample terms: {feature_names[:10]}\")\n",
    "        \n",
    "        # Create DataFrame with TF-IDF features\n",
    "        tfidf_df = pd.DataFrame(\n",
    "            tfidf_matrix.toarray(),\n",
    "            index=df.index,\n",
    "            columns=[f'tfidf_{feat}' for feat in feature_names]\n",
    "        )\n",
    "        \n",
    "        return pd.concat([df, tfidf_df], axis=1)\n",
    "    \n",
    "    def transform_query(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Transform query text to TF-IDF vector\"\"\"\n",
    "        processed_query = self.preprocessor.preprocess_text(query)\n",
    "        if self.debug:\n",
    "            self.logger.debug(f\"\\nProcessed query: '{processed_query}'\")\n",
    "        \n",
    "        query_vector = self.vectorizer.transform([processed_query])\n",
    "        query_array = query_vector.toarray()[0]\n",
    "        \n",
    "        if self.debug:\n",
    "            self._log_query_stats(query_array)\n",
    "            \n",
    "        return query_array\n",
    "    \n",
    "    def _log_query_stats(self, query_array: np.ndarray):\n",
    "        \"\"\"Log query vector statistics if debug is enabled\"\"\"\n",
    "        vocabulary = self.vectorizer.get_feature_names_out()\n",
    "        non_zero_indices = np.nonzero(query_array)[0]\n",
    "        \n",
    "        self.logger.debug(f\"\\nQuery stats:\")\n",
    "        self.logger.debug(f\"Vector norm: {np.linalg.norm(query_array):.6f}\")\n",
    "        self.logger.debug(f\"Non-zero terms: {len(non_zero_indices)}\")\n",
    "        \n",
    "        if len(non_zero_indices) > 0:\n",
    "            self.logger.debug(\"Matching terms:\")\n",
    "            for idx in non_zero_indices:\n",
    "                self.logger.debug(f\"  {vocabulary[idx]}: {query_array[idx]:.6f}\")\n",
    "    \n",
    "    def find_similar_documents(self, query_vector: np.ndarray, \n",
    "                             document_vectors: np.ndarray,\n",
    "                             df_indices: List[Any],\n",
    "                             top_k: int = 5) -> Tuple[List[Any], List[float]]:\n",
    "        \"\"\"\n",
    "        Find most similar documents using cosine similarity\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (document indices, similarity scores)\n",
    "            Empty lists if no similar documents found\n",
    "        \"\"\"\n",
    "        if len(df_indices) != document_vectors.shape[0]:\n",
    "            raise ValueError(\"Number of indices must match number of document vectors\")\n",
    "            \n",
    "        # Compute similarities\n",
    "        query_norm = np.linalg.norm(query_vector)\n",
    "        doc_norms = np.linalg.norm(document_vectors, axis=1)\n",
    "        \n",
    "        # Handle zero vectors\n",
    "        if query_norm == 0:\n",
    "            self.logger.warning(\"Query vector is zero - no matches possible\")\n",
    "            return [], []\n",
    "            \n",
    "        # Compute cosine similarities efficiently\n",
    "        similarities = np.zeros(len(document_vectors))\n",
    "        non_zero_docs = doc_norms > 0\n",
    "        \n",
    "        if not any(non_zero_docs):\n",
    "            self.logger.warning(\"No valid document vectors found\")\n",
    "            return [], []\n",
    "            \n",
    "        # Vectorized similarity computation\n",
    "        similarities[non_zero_docs] = (\n",
    "            document_vectors[non_zero_docs] @ query_vector / \n",
    "            (doc_norms[non_zero_docs] * query_norm)\n",
    "        )\n",
    "        \n",
    "        # Get top k results\n",
    "        valid_similarities = similarities[~np.isnan(similarities)]\n",
    "        if len(valid_similarities) == 0:\n",
    "            return [], []\n",
    "            \n",
    "        top_k = min(top_k, len(valid_similarities))\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        return [df_indices[i] for i in top_indices], similarities[top_indices].tolist()\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save model state\"\"\"\n",
    "        model_data = {\n",
    "            'vectorizer': self.vectorizer,\n",
    "            'preprocessor': self.preprocessor\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model(filepath: str, debug: bool = False) -> 'DocumentSimilarity':\n",
    "        \"\"\"Load saved model\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        processor = DocumentSimilarity(\n",
    "            max_features=model_data['vectorizer'].max_features,\n",
    "            debug=debug\n",
    "        )\n",
    "        processor.vectorizer = model_data['vectorizer']\n",
    "        processor.preprocessor = model_data['preprocessor']\n",
    "        return processor\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create processor with debugging disabled\n",
    "    processor = DocumentSimilarity(max_features=1000, debug=False)\n",
    "    \n",
    "    # Example data\n",
    "    df = pd.DataFrame({\n",
    "        'text_to_embed': [\n",
    "            \"This is a sample document about machine learning\",\n",
    "            \"Another document discussing data science\",\n",
    "            \"A third document about artificial intelligence\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Process documents\n",
    "    processed_df = processor.fit_transform_documents(df, 'text_to_embed')\n",
    "    \n",
    "    # Example query\n",
    "    query = \"machine learning and data science\"\n",
    "    query_vector = processor.transform_query(query)\n",
    "    \n",
    "    # Get document vectors\n",
    "    tfidf_columns = [col for col in processed_df.columns if col.startswith('tfidf_')]\n",
    "    document_vectors = processed_df[tfidf_columns].values\n",
    "    \n",
    "    # Find similar documents\n",
    "    similar_indices, similarities = processor.find_similar_documents(\n",
    "        query_vector, \n",
    "        document_vectors,\n",
    "        df_indices=processed_df.index.tolist()\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    if similar_indices:\n",
    "        for idx, sim in zip(similar_indices, similarities):\n",
    "            print(f\"Document: {df.loc[idx, 'text_to_embed']}\")\n",
    "            print(f\"Similarity: {sim:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 175 entries, 292 to 204769\n",
      "Data columns (total 16 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   abn                               175 non-null    int64  \n",
      " 1   charity name                      175 non-null    object \n",
      " 2   how purposes were pursued         172 non-null    object \n",
      " 3   total full time equivalent staff  175 non-null    float64\n",
      " 4   staff - volunteers                175 non-null    int64  \n",
      " 5   Program name                      175 non-null    object \n",
      " 6   Classification                    175 non-null    object \n",
      " 7   Charity weblink                   175 non-null    object \n",
      " 8   location_number                   175 non-null    int64  \n",
      " 9   operating_location                175 non-null    object \n",
      " 10  latitude                          175 non-null    float64\n",
      " 11  longitude                         175 non-null    float64\n",
      " 12  distance                          175 non-null    float64\n",
      " 13  id                                175 non-null    object \n",
      " 14  text_to_embed                     175 non-null    object \n",
      " 15  embedding                         0 non-null      object \n",
      "dtypes: float64(4), int64(3), object(9)\n",
      "memory usage: 27.3+ KB\n"
     ]
    }
   ],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Southern Youth & Family Services Limited | Family Counselling Project | Wollongong NSW, Australia\n",
      "Text: ID:Southern Youth & Family Services Limited | Family Counselling Project | Wollongong NSW, Australia - We provide support and assistance to children, young people, adults and families who are disadvan...\n",
      "Similarity: 0.2839\n",
      "\n",
      "Document Southern Youth & Family Services Limited | Cringila Community Development; Playgroup; Southern Suburbs Preschool | Cringila NSW, Australia\n",
      "Text: ID:Southern Youth & Family Services Limited | Cringila Community Development; Playgroup; Southern Suburbs Preschool | Cringila NSW, Australia - We provide support and assistance to children, young peo...\n",
      "Similarity: 0.2732\n",
      "\n",
      "Document Southern Youth & Family Services Limited | Reconnect Program and Family Mental Health Support Service (CAFS) | Wollongong NSW, Australia\n",
      "Text: ID:Southern Youth & Family Services Limited | Reconnect Program and Family Mental Health Support Service (CAFS) | Wollongong NSW, Australia - We provide support and assistance to children, young peopl...\n",
      "Similarity: 0.2708\n",
      "\n",
      "Document Wollongong Homeless Hub and Housing Services | Wollongong Homeless Hub | Wollongong NSW, Australia\n",
      "Text: ID:Wollongong Homeless Hub and Housing Services | Wollongong Homeless Hub | Wollongong NSW, Australia - We provided drop in support, emergency accommodation, transitional accommodation, case managemen...\n",
      "Similarity: 0.2055\n",
      "\n",
      "Document St John the Baptist Serbian Orthodox Parish and Church Community | Vespers religious service | 82 Kenny Street, Wollongong NSW, Australia\n",
      "Text: ID:St John the Baptist Serbian Orthodox Parish and Church Community | Vespers religious service | 82 Kenny Street, Wollongong NSW, Australia - Religious services on daily basis...\n",
      "Similarity: 0.1907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize with debugging off\n",
    "processor = DocumentSimilarity(\n",
    "    max_features=1000,\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    debug=True  # Set to True if you need debugging output\n",
    ")\n",
    "\n",
    "# Process documents\n",
    "processed_df = processor.fit_transform_documents(filtered_data, 'text_to_embed')\n",
    "\n",
    "# Transform query\n",
    "query = \"A non-religious or sectarian soup kitchen for homeless or disadvantaged people\"\n",
    "query_vector = processor.transform_query(query)\n",
    "\n",
    "# Get document vectors\n",
    "tfidf_columns = [col for col in processed_df.columns if col.startswith('tfidf_')]\n",
    "document_vectors = processed_df[tfidf_columns].values\n",
    "\n",
    "# Find similar documents\n",
    "similar_indices, similarities = processor.find_similar_documents(\n",
    "    query_vector, \n",
    "    document_vectors,\n",
    "    df_indices=processed_df.index.tolist()\n",
    ")\n",
    "\n",
    "# Print results\n",
    "if similar_indices:\n",
    "    for idx, similarity in zip(similar_indices, similarities):\n",
    "        print(f\"Document {filtered_data.loc[idx, 'id']}\")\n",
    "        print(f\"Text: {filtered_data.loc[idx, 'text_to_embed'][:200]}...\")\n",
    "        print(f\"Similarity: {similarity:.4f}\\n\")\n",
    "else:\n",
    "    print(\"No similar documents found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56904, 22575, 56913, 26164, 16352]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28385910307126794,\n",
       " 0.2731521715021081,\n",
       " 0.27081219353533453,\n",
       " 0.20546684110403923,\n",
       " 0.19069065469080873]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geoff\\dev\\buildmas24\\src\\data_processing.py:13: DtypeWarning: Columns (78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  charities_df = pd.read_csv(data_dir / \"datadotgov_ais22.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Charity: St Thomas' Anglican Church Port Macquarie\n",
      "Similarity: 0.668\n",
      "Program: St Thomas Soup kitchen\n",
      "Location: St Thomas' Anglican Church, 50 Hay Street, Port Macquarie NSW, Australia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Charity: Street Mission Incorporated\n",
      "Similarity: 0.648\n",
      "Program: Soup Kitchen\n",
      "Location: Dee Why NSW, Australia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Charity: Street Mission Incorporated\n",
      "Similarity: 0.648\n",
      "Program: Soup Kitchen\n",
      "Location: Balgowlah NSW, Australia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Charity: Light House Community Services Inc\n",
      "Similarity: 0.588\n",
      "Program: Light House Soup Kitchen\n",
      "Location: Gold Coast QLD, Australia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Charity: Southcare Community Care Inc\n",
      "Similarity: 0.568\n",
      "Program: Soup Kitchen\n",
      "Location: Frankston North VIC, Australia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Charity: Diocese of Maitland-Newcastle Development and Relief Agency\n",
      "Similarity: 0.567\n",
      "Program: Toronto Community Kitchen\n",
      "Location: Lake Macquarie, New South Wales, Australia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Charity: Fishers Of Men\n",
      "Similarity: 0.552\n",
      "Program: Soup Kitchen\n",
      "Location: Brisbane QLD, Australia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Charity: CATHOLIC PARISH ST CANICE ELIZABETH BAY AS THE OPERATOR OF A PBI\n",
      "Similarity: 0.529\n",
      "Program: Canice's Kitchen\n",
      "Location: 28 Roslyn Street, Rushcutters Bay NSW, Australia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Charity: Manna4life Ltd\n",
      "Similarity: 0.512\n",
      "Program: Soup Kitchen Meal Sevice\n",
      "Location: 34 Oak Avenue, Doveton VIC, Australia\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Charity: Manna4life Ltd\n",
      "Similarity: 0.512\n",
      "Program: Soup Kitchen Meal Sevice\n",
      "Location: 2C Cuthbert Rd, Reservoir VIC, Australia\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# this code was modularised for the streamlit app and can be accessed as follows;\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the absolute path to the project root (parent of notebooks directory)\n",
    "project_root = Path().absolute().parent\n",
    "\n",
    "# Add project root to Python path so src can be found\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "\n",
    "# Import our modules\n",
    "from src.similarity_manager import DocumentSimilarityManager\n",
    "from src.data_processing import load_charity_data, prepare_text_field\n",
    "\n",
    "# Initialize the similarity manager\n",
    "manager = DocumentSimilarityManager(\n",
    "    model_dir=\"../data/models\",\n",
    "    processed_dir=\"../data/processed\",\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "# Load existing model or train new one\n",
    "if not manager.load_model():\n",
    "    print(\"Training new model...\")\n",
    "    df = load_charity_data()\n",
    "    manager.train_model(df, text_column='text_to_embed')\n",
    "    print(\"Model trained!\")\n",
    "\n",
    "\n",
    "# Hit the model with a query\n",
    "query = \"A soup kitchen for homeless people\"\n",
    "top_k = 10\n",
    "similar_indices, similarities = manager.find_similar(query, top_k=top_k)\n",
    "\n",
    "if not similar_indices:\n",
    "    print(\"No matches found\")\n",
    "\n",
    "df = load_charity_data()\n",
    "for idx, sim in zip(similar_indices, similarities):\n",
    "    print(f\"\\nCharity: {df.loc[idx, 'charity name']}\")\n",
    "    print(f\"Similarity: {sim:.3f}\")\n",
    "    print(f\"Program: {df.loc[idx, 'Program name']}\")\n",
    "    print(f\"Location: {df.loc[idx, 'operating_location']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt engineeering and tests - `pytest` or `projit`\n",
    "Tests could include:\n",
    "- valid json\n",
    "- json contains keys of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping - `folium`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tying together - `streamlit`\n",
    "- Mobile experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting user location\n",
    "# https://github.com/aghasemi/streamlit_js_eval?tab=readme-ov-file\n",
    "# though note you'll need an error route - i.e. agent asks for locatioon if it fails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
